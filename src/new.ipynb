{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using Statistics\n",
    "using Random\n",
    "using Optimisers\n",
    "using LaplaceRedux\n",
    "using Plots\n",
    "using TaijaPlotting\n",
    "include(\"laplace_utils.jl\")\n",
    "Random.seed!(42)\n",
    "\n",
    "train_data = MLDatasets.MNIST(:train)\n",
    "test_data = MLDatasets.MNIST(:test)\n",
    "\n",
    "X_train = reshape(train_data.features, :, 60000)\n",
    "y_train = Flux.onehotbatch(train_data.targets, 0:9)\n",
    "X_test = reshape(test_data.features, :, 10000)\n",
    "y_test = Flux.onehotbatch(test_data.targets, 0:9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (784, 60000), type: Float32\n",
      "X_train min: 0.0, max: 1.0\n",
      "X_test shape: (784, 10000), type: Float32\n",
      "X_test min: 0.0, max: 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = Float32.(X_train)\n",
    "X_test = Float32.(X_test)\n",
    "\n",
    "println(\"X_train shape: \", size(X_train), \", type: \", eltype(X_train))\n",
    "println(\"X_train min: \", minimum(X_train), \", max: \", maximum(X_train))\n",
    "println(\"X_test shape: \", size(X_test), \", type: \", eltype(X_test))\n",
    "println(\"X_test min: \", minimum(X_test), \", max: \", maximum(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layers = ((weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(784, 128, relu, init=Flux.glorot_uniform),\n",
    "    Dense(128, 64, relu, init=Flux.glorot_uniform),\n",
    "    Dense(64, 10, init=Flux.glorot_uniform)\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.crossentropy(softmax(model(x)), y)\n",
    "opt = Optimisers.Adam(0.01)\n",
    "\n",
    "state = Optimisers.setup(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Vector{Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}}:\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 1; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [1; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 1; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 1;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 1;;])\n",
       " ⋮\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 1; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 1;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [1; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 1; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = [(X_train[:, i:i], y_train[:, i:i]) for i in 1:60000]\n",
    "data_test = [(X_test[:, i:i], y_test[:, i:i]) for i in 1:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.23097360283851243, test loss = 0.1283626\n",
      "Epoch 2: Loss = 0.11753735999300727, test loss = 0.13265969\n",
      "Epoch 3: Loss = 0.09541607318894822, test loss = 0.12968938\n",
      "Epoch 4: Loss = 0.08396283034950114, test loss = 0.12967698\n",
      "Epoch 5: Loss = 0.07950870870157822, test loss = 0.16026653\n",
      "Epoch 6: Loss = 0.07453342231570372, test loss = 0.15758485\n",
      "Epoch 7: Loss = 0.0679159811352874, test loss = 0.12566106\n",
      "Epoch 8: Loss = 0.06486266567200513, test loss = 0.11975501\n",
      "Epoch 9: Loss = 0.06111072177397035, test loss = 0.13981858\n",
      "Epoch 10: Loss = 0.06321341280208857, test loss = 0.12926953\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "for epoch in 1:num_epochs\n",
    "    perm = randperm(60000)\n",
    "    X_train = X_train[:, perm]\n",
    "    y_train = y_train[:, perm]\n",
    "    data_train = data_train[perm]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i in 1:batch_size:60000\n",
    "        x_batch = X_train[:, i:min(i+batch_size-1, 60000)]\n",
    "        y_batch = y_train[:, i:min(i+batch_size-1, 60000)]\n",
    "        \n",
    "        (l, grads) = Flux.withgradient(model) do m\n",
    "            Flux.crossentropy(softmax(m(x_batch)), y_batch)\n",
    "        end\n",
    "        \n",
    "        state, model = Optimisers.update(state, model, grads[1])\n",
    "        \n",
    "        total_loss += l\n",
    "        num_batches += 1\n",
    "    end\n",
    "    \n",
    "    train_loss = total_loss / num_batches\n",
    "    test_loss = loss(X_test, y_test)\n",
    "    println(\"Epoch $epoch: Loss = $train_loss, test loss = $test_loss\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP ACC: 0.9695\n"
     ]
    }
   ],
   "source": [
    "y_pred = softmax(model(X_test))\n",
    "y_pred_labels = [argmax(y_pred[:, i]) - 1 for i in 1:10000]\n",
    "y_true_labels = test_data.targets\n",
    "accuracy = mean(y_pred_labels .== y_true_labels)\n",
    "println(\"MAP ACC: $accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_laplace (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_laplace(model, data_train, X_test, y_test, hessian_type, la_name)\n",
    "    if CUDA.has_cuda()\n",
    "        model = gpu(model)\n",
    "        X_test = gpu(X_test)\n",
    "        y_test = gpu(y_test)\n",
    "        data_train = [(gpu(x), gpu(y)) for (x, y) in data_train]\n",
    "    else\n",
    "        println(\"CUDA Not Found, using CPU\")\n",
    "    end\n",
    "\n",
    "    println(\"Apply $la_name (Hessian: $hessian_type)...\")\n",
    "    \n",
    "    la = Laplace(model; likelihood=:classification, hessian_structure=hessian_type, subset_of_weights=:last_layer)\n",
    "    fit!(la, data_train)\n",
    "    optimize_prior!(la; verbosity=1, n_steps=100)\n",
    "    \n",
    "    y_pred_la = predict(la, X_test, ret_distr=true)\n",
    "\n",
    "    y_pred_labels_la = [argmax(p.p) - 1 for p in y_pred_la]\n",
    "    accuracy_la = mean(y_pred_labels_la .== vec(y_test))\n",
    "    println(\"$la_name test set acc: $accuracy_la\")\n",
    "\n",
    "    _labels = 0:9 \n",
    "    for target in _labels\n",
    "        bernoulli_distributions = [LaplaceRedux.Bernoulli(p.p[target + 1]) for p in y_pred_la]\n",
    "        y_onehot_test = Flux.unstack(Array(y_test)', 1)\n",
    "        plt = Calibration_Plot(la, [y[target + 1] for y in y_onehot_test], bernoulli_distributions; n_bins=10)\n",
    "        savefig(plt, \"calibration_class_$(target)_$(la_name).png\")\n",
    "        println(\" $target has a calibration curve: calibration_class_$(target)_$(la_name).png\")\n",
    "    end\n",
    "\n",
    "    confidences = [maximum(p.p) for p in y_pred_la]\n",
    "    mean_confidence = mean(confidences)\n",
    "    println(\"AVG confidence: $mean_confidence\")\n",
    "\n",
    "    y_true = [argmax(Array(y_test)[:, i]) - 1 for i in 1:size(y_test, 2)]\n",
    "    auroc_list = Float64[]\n",
    "    for class_idx in 0:9\n",
    "        y_true_bin = [yt == class_idx ? 1 : 0 for yt in y_true]\n",
    "        y_score = [p.p[class_idx + 1] for p in y_pred_la]\n",
    "        roc_obj = roc(y_score, y_true_bin)\n",
    "        auc = auc(roc_obj)\n",
    "        push!(auroc_list, auc)\n",
    "        println(\"Class $class_idx with AUROC: $auc\")\n",
    "    end\n",
    "    mean_auroc = mean(auroc_list)\n",
    "    println(\"AUROC: $mean_auroc\")\n",
    "    \n",
    "    println(\"$la_name DONE\")\n",
    "    return la\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "la_kron = apply_laplace(model, data_train, X_test, y_test, :kron, \"LA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_full = apply_laplace(model, data_train, X_test, y_test, :full, \"LA*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
