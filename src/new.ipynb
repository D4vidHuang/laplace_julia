{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using Statistics\n",
    "using Random\n",
    "using Optimisers\n",
    "using LaplaceRedux\n",
    "using Plots\n",
    "using TaijaPlotting\n",
    "include(\"laplace_utils.jl\")\n",
    "Random.seed!(42)\n",
    "\n",
    "train_data = MLDatasets.MNIST(:train)\n",
    "test_data = MLDatasets.MNIST(:test)\n",
    "\n",
    "X_train = reshape(train_data.features, :, 60000)\n",
    "y_train = Flux.onehotbatch(train_data.targets, 0:9)\n",
    "X_test = reshape(test_data.features, :, 10000)\n",
    "y_test = Flux.onehotbatch(test_data.targets, 0:9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (784, 60000), type: Float32\n",
      "X_train min: 0.0, max: 1.0\n",
      "X_test shape: (784, 10000), type: Float32\n",
      "X_test min: 0.0, max: 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = Float32.(X_train)\n",
    "X_test = Float32.(X_test)\n",
    "\n",
    "println(\"X_train shape: \", size(X_train), \", type: \", eltype(X_train))\n",
    "println(\"X_train min: \", minimum(X_train), \", max: \", maximum(X_train))\n",
    "println(\"X_test shape: \", size(X_test), \", type: \", eltype(X_test))\n",
    "println(\"X_test min: \", minimum(X_test), \", max: \", maximum(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layers = ((weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ())),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(784, 128, relu, init=Flux.glorot_uniform),\n",
    "    Dense(128, 64, relu, init=Flux.glorot_uniform),\n",
    "    Dense(64, 10, init=Flux.glorot_uniform)\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.crossentropy(softmax(model(x)), y)\n",
    "opt = Optimisers.Adam(0.01)\n",
    "\n",
    "state = Optimisers.setup(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Vector{Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}}:\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 1; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [1; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 1; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 1;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 1;;])\n",
       " ⋮\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 1; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 1;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [1; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 1; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])\n",
       " ([0.0; 0.0; … ; 0.0; 0.0;;], [0; 0; … ; 0; 0;;])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = [(X_train[:, i:i], y_train[:, i:i]) for i in 1:60000]\n",
    "data_test = [(X_test[:, i:i], y_test[:, i:i]) for i in 1:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.2311840876380899, test loss = 0.14064391\n",
      "Epoch 2: Loss = 0.12437022879306696, test loss = 0.14867124\n",
      "Epoch 3: Loss = 0.09940545714057203, test loss = 0.12151123\n",
      "Epoch 4: Loss = 0.0909477540774386, test loss = 0.15791947\n",
      "Epoch 5: Loss = 0.08758597794189446, test loss = 0.11913164\n",
      "Epoch 6: Loss = 0.07573572247932508, test loss = 0.121748075\n",
      "Epoch 7: Loss = 0.07286197372497732, test loss = 0.12765886\n",
      "Epoch 8: Loss = 0.07262363233295743, test loss = 0.13884676\n",
      "Epoch 9: Loss = 0.06302816210426827, test loss = 0.14173672\n",
      "Epoch 10: Loss = 0.06396205308335795, test loss = 0.13160942\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "for epoch in 1:num_epochs\n",
    "    perm = randperm(60000)\n",
    "    X_train = X_train[:, perm]\n",
    "    y_train = y_train[:, perm]\n",
    "    data_train = data_train[perm]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i in 1:batch_size:60000\n",
    "        x_batch = X_train[:, i:min(i+batch_size-1, 60000)]\n",
    "        y_batch = y_train[:, i:min(i+batch_size-1, 60000)]\n",
    "        \n",
    "        (l, grads) = Flux.withgradient(model) do m\n",
    "            Flux.crossentropy(softmax(m(x_batch)), y_batch)\n",
    "        end\n",
    "        \n",
    "        state, model = Optimisers.update(state, model, grads[1])\n",
    "        \n",
    "        total_loss += l\n",
    "        num_batches += 1\n",
    "    end\n",
    "    \n",
    "    train_loss = total_loss / num_batches\n",
    "    test_loss = loss(X_test, y_test)\n",
    "    println(\"Epoch $epoch: Loss = $train_loss, test loss = $test_loss\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP ACC: 0.973\n"
     ]
    }
   ],
   "source": [
    "y_pred = softmax(model(X_test))\n",
    "y_pred_labels = [argmax(y_pred[:, i]) - 1 for i in 1:10000]\n",
    "y_true_labels = test_data.targets\n",
    "accuracy = mean(y_pred_labels .== y_true_labels)\n",
    "println(\"MAP ACC: $accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_laplace (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_laplace(model, data_train, X_test, y_test, hessian_type, la_name)\n",
    "    if CUDA.has_cuda()\n",
    "        model = gpu(model)\n",
    "        X_test = gpu(X_test)\n",
    "        y_test = gpu(y_test)\n",
    "        data_train = [(gpu(x), gpu(y)) for (x, y) in data_train]\n",
    "    else\n",
    "        println(\"CUDA Not Found, using CPU\")\n",
    "    end\n",
    "\n",
    "    println(\"Apply $la_name (Hessian: $hessian_type)...\")\n",
    "    \n",
    "    la = Laplace(model; likelihood=:classification, hessian_structure=hessian_type, subset_of_weights=:last_layer)\n",
    "    fit!(la, data_train)\n",
    "    optimize_prior!(la; verbosity=1, n_steps=100)\n",
    "    \n",
    "    y_pred_la = predict(la, X_test, ret_distr=true)\n",
    "\n",
    "    y_pred_labels_la = [argmax(p.p) - 1 for p in y_pred_la]\n",
    "    accuracy_la = mean(y_pred_labels_la .== vec(y_test))\n",
    "    println(\"$la_name test set acc: $accuracy_la\")\n",
    "\n",
    "    _labels = 0:9 \n",
    "    for target in _labels\n",
    "        bernoulli_distributions = [LaplaceRedux.Bernoulli(p.p[target + 1]) for p in y_pred_la]\n",
    "        y_onehot_test = Flux.unstack(Array(y_test)', 1)\n",
    "        plt = Calibration_Plot(la, [y[target + 1] for y in y_onehot_test], bernoulli_distributions; n_bins=10)\n",
    "        savefig(plt, \"calibration_class_$(target)_$(la_name).png\")\n",
    "        println(\" $target has a calibration curve: calibration_class_$(target)_$(la_name).png\")\n",
    "    end\n",
    "\n",
    "    confidences = [maximum(p.p) for p in y_pred_la]\n",
    "    mean_confidence = mean(confidences)\n",
    "    println(\"AVG confidence: $mean_confidence\")\n",
    "\n",
    "    y_true = [argmax(Array(y_test)[:, i]) - 1 for i in 1:size(y_test, 2)]\n",
    "    auroc_list = Float64[]\n",
    "    for class_idx in 0:9\n",
    "        y_true_bin = [yt == class_idx ? 1 : 0 for yt in y_true]\n",
    "        y_score = [p.p[class_idx + 1] for p in y_pred_la]\n",
    "        roc_obj = roc(y_score, y_true_bin)\n",
    "        auc = auc(roc_obj)\n",
    "        push!(auroc_list, auc)\n",
    "        println(\"Class $class_idx with AUROC: $auc\")\n",
    "    end\n",
    "    mean_auroc = mean(auroc_list)\n",
    "    println(\"AUROC: $mean_auroc\")\n",
    "    \n",
    "    println(\"$la_name DONE\")\n",
    "    return la\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Not Found, using CPU\n",
      "Apply LA (Hessian: kron)...\n"
     ]
    },
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: matrix is not square: dimensions are (10, 64)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: matrix is not square: dimensions are (10, 64)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0 [inlined]\n",
      "  [2] _pullback(ctx::Zygote.Context{true}, f::typeof(throw), args::DimensionMismatch)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:91\n",
      "  [3] checksquare\n",
      "    @ ~/.julia/juliaup/julia-1.11.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.11/LinearAlgebra/src/LinearAlgebra.jl:302 [inlined]\n",
      "  [4] _pullback(ctx::Zygote.Context{true}, f::typeof(LinearAlgebra.checksquare), args::Matrix{Float64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      "  [5] gees!\n",
      "    @ ~/.julia/juliaup/julia-1.11.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.11/LinearAlgebra/src/lapack.jl:6436 [inlined]\n",
      "  [6] _pullback(::Zygote.Context{true}, ::typeof(LinearAlgebra.LAPACK.gees!), ::Char, ::Matrix{Float64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      "  [7] schur!\n",
      "    @ ~/.julia/juliaup/julia-1.11.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.11/LinearAlgebra/src/schur.jl:103 [inlined]\n",
      "  [8] _pullback(ctx::Zygote.Context{true}, f::typeof(LinearAlgebra.schur!), args::Matrix{Float64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      "  [9] schur\n",
      "    @ ~/.julia/juliaup/julia-1.11.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.11/LinearAlgebra/src/schur.jl:157 [inlined]\n",
      " [10] _pullback(ctx::Zygote.Context{true}, f::typeof(LinearAlgebra.schur), args::Matrix{Float64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [11] log\n",
      "    @ ~/.julia/juliaup/julia-1.11.5+0.aarch64.apple.darwin14/share/julia/stdlib/v1.11/LinearAlgebra/src/dense.jl:834 [inlined]\n",
      " [12] _pullback(ctx::Zygote.Context{true}, f::typeof(log), args::Matrix{Float64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [13] logdetblock\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/kronecker/utils.jl:156 [inlined]\n",
      " [14] _pullback(::Zygote.Context{true}, ::typeof(LaplaceRedux.logdetblock), ::Tuple{LinearAlgebra.Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}, LinearAlgebra.Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}}, ::Float64)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [15] #50\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/kronecker/utils.jl:165 [inlined]\n",
      " [16] _pullback(ctx::Zygote.Context{true}, f::LaplaceRedux.var\"#50#51\"{LaplaceRedux.KronDecomposed}, args::Tuple{LinearAlgebra.Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}, LinearAlgebra.Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [17] rrule_via_ad(::Zygote.ZygoteRuleConfig{Zygote.Context{true}}, ::Function, ::Vararg{Any}; kwargs::@Kwargs{})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/chainrules.jl:261\n",
      " [18] rrule_via_ad\n",
      "    @ ~/.julia/packages/Zygote/zowwZ/src/compiler/chainrules.jl:249 [inlined]\n",
      " [19] #727\n",
      "    @ ./none:0 [inlined]\n",
      " [20] iterate\n",
      "    @ ./generator.jl:48 [inlined]\n",
      " [21] collect(itr::Base.Generator{Vector{Tuple{LinearAlgebra.Eigen, LinearAlgebra.Eigen}}, ChainRules.var\"#727#732\"{Zygote.ZygoteRuleConfig{Zygote.Context{true}}, LaplaceRedux.var\"#50#51\"{LaplaceRedux.KronDecomposed}}})\n",
      "    @ Base ./array.jl:791\n",
      " [22] rrule(config::Zygote.ZygoteRuleConfig{Zygote.Context{true}}, ::typeof(sum), f::LaplaceRedux.var\"#50#51\"{LaplaceRedux.KronDecomposed}, xs::Vector{Tuple{LinearAlgebra.Eigen, LinearAlgebra.Eigen}}; dims::Function)\n",
      "    @ ChainRules ~/.julia/packages/ChainRules/Q16hj/src/rulesets/Base/mapreduce.jl:102\n",
      " [23] rrule\n",
      "    @ ~/.julia/packages/ChainRules/Q16hj/src/rulesets/Base/mapreduce.jl:76 [inlined]\n",
      " [24] chain_rrule\n",
      "    @ ~/.julia/packages/Zygote/zowwZ/src/compiler/chainrules.jl:224 [inlined]\n",
      " [25] macro expansion\n",
      "    @ ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0 [inlined]\n",
      " [26] _pullback(::Zygote.Context{true}, ::typeof(sum), ::LaplaceRedux.var\"#50#51\"{LaplaceRedux.KronDecomposed}, ::Vector{Tuple{LinearAlgebra.Eigen, LinearAlgebra.Eigen}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:91\n",
      " [27] logdet\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/kronecker/utils.jl:165 [inlined]\n",
      " [28] _pullback(ctx::Zygote.Context{true}, f::typeof(LinearAlgebra.logdet), args::LaplaceRedux.KronDecomposed)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [29] log_det_posterior_precision\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/utils.jl:157 [inlined]\n",
      " [30] _pullback(ctx::Zygote.Context{true}, f::typeof(LaplaceRedux.log_det_posterior_precision), args::Laplace)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [31] log_det_ratio\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/utils.jl:142 [inlined]\n",
      " [32] _pullback(ctx::Zygote.Context{true}, f::typeof(LaplaceRedux.log_det_ratio), args::Laplace)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [33] #log_marginal_likelihood#9\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/utils.jl:133 [inlined]\n",
      " [34] _pullback(::Zygote.Context{true}, ::LaplaceRedux.var\"##log_marginal_likelihood#9\", ::Float64, ::Float64, ::typeof(LaplaceRedux.log_marginal_likelihood), ::Laplace)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [35] log_marginal_likelihood\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/utils.jl:115 [inlined]\n",
      " [36] _pullback(::Zygote.Context{true}, ::typeof(Core.kwcall), ::@NamedTuple{P₀::Float64, σ::Float64}, ::typeof(LaplaceRedux.log_marginal_likelihood), ::Laplace)\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [37] loss\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:36 [inlined]\n",
      " [38] _pullback(::Zygote.Context{true}, ::LaplaceRedux.var\"#loss#28\"{Laplace}, ::Vector{Float64}, ::Vector{Float64})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [39] #27\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:41 [inlined]\n",
      " [40] _pullback(::Zygote.Context{true}, ::LaplaceRedux.var\"#27#29\"{LaplaceRedux.var\"#loss#28\"{Laplace}, Vector{Float64}, Vector{Float64}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface2.jl:0\n",
      " [41] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface.jl:465\n",
      " [42] gradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/zowwZ/src/compiler/interface.jl:147\n",
      " [43] optimize_prior!(la::Laplace; n_steps::Int64, lr::Float64, λinit::Nothing, σinit::Nothing, verbosity::Int64, tune_σ::Bool)\n",
      "    @ LaplaceRedux ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:40\n",
      " [44] optimize_prior!\n",
      "    @ ~/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:11 [inlined]\n",
      " [45] apply_laplace(model::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, data_train::Vector{Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}}, X_test::Matrix{Float32}, y_test::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, hessian_type::Symbol, la_name::String)\n",
      "    @ Main ~/Desktop/laplace_julia/src/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X10sZmlsZQ==.jl:15\n",
      " [46] top-level scope\n",
      "    @ ~/Desktop/laplace_julia/src/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X11sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "\n",
    "la_kron = apply_laplace(model, data_train, X_test, y_test, :kron, \"LA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Not Found, using CPU\n",
      "Apply LA* (Hessian: full)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 10: P₀=2.6899557264887024, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(exp.(logP₀), exp.(logσ)) = 4032.044273171899\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1454.567556056358\n",
      "Scatter: 114.6564635878981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 20: P₀=6.463462150737228, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(exp.(logP₀), exp.(logσ)) = 3962.522332576617\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1154.6820306446407\n",
      "Scatter: 275.4981078090516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 30: P₀=9.866637217052713, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(exp.(logP₀), exp.(logσ)) = 3968.009265947931\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1020.5992281522026\n",
      "Scatter: 420.5547770441178\n",
      "loss(exp.(logP₀), exp.(logσ)) = 3961.831636643865"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 40: P₀=8.406270529080444, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1070.4905256251393\n",
      "Scatter: 358.3082209630488\n",
      "loss(exp.(logP₀), exp.(logσ)) = 3961.2400819565346\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1133.8059781576146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 50: P₀=6.8930695238935185, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scatter: 293.80965905591324\n",
      "loss(exp.(logP₀), exp.(logσ)) = 3960.9084007932775\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1125.1747560044432\n",
      "Scatter: 301.7775188825699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 60: P₀=7.0800035134644, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n",
      "┌ Info: Iteration 70: P₀=7.692342041442518, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(exp.(logP₀), exp.(logσ)) = 3960.6730100758805\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1098.6037047974637\n",
      "Scatter: 327.87778865475525\n",
      "loss(exp.(logP₀), exp.(logσ)) = 3960.6502016166787\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1100.3669542414607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 80: P₀=7.649904220863293, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scatter: 326.06892229235496\n",
      "loss(exp.(logP₀), exp.(logσ)) = 3960.6255285953903\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1110.2522436085155\n",
      "Scatter: 316.13428688272376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Iteration 90: P₀=7.4168277018912265, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n",
      "┌ Info: Iteration 100: P₀=7.458982717172642, σ=1.0\n",
      "└ @ LaplaceRedux /Users/davidhuang/.julia/packages/LaplaceRedux/1bSKz/src/baselaplace/optimize_prior.jl:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(exp.(logP₀), exp.(logσ)) = 3960.617109552105\n",
      "Log likelihood: -3247.432263349771\n",
      "Log det ratio: 1108.4385934305537\n",
      "Scatter: 317.93109897411466\n",
      "LA* test set acc: 0.09961\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `Calibration_Plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Calibration_Plot` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] apply_laplace(model::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, data_train::Vector{Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}}, X_test::Matrix{Float32}, y_test::OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}, hessian_type::Symbol, la_name::String)\n",
      "   @ Main ~/Desktop/laplace_julia/src/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X10sZmlsZQ==.jl:27\n",
      " [2] top-level scope\n",
      "   @ ~/Desktop/laplace_julia/src/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X12sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "la_full = apply_laplace(model, data_train, X_test, y_test, :full, \"LA*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
