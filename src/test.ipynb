{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载MNIST数据集...\n",
      "验证原始训练数据：\n",
      "X_train shape: (784, 60000), type: Float32\n",
      "X_train min: 0.0, max: 1.0\n",
      "验证原始测试数据：\n",
      "X_test shape: (784, 10000), type: Float32\n",
      "X_test min: 0.0, max: 1.0\n",
      "预处理后训练数据：\n",
      "X_train shape: (784, 60000), type: Float32\n",
      "X_train min: 0.0, max: 1.0\n",
      "预处理后测试数据：\n",
      "X_test shape: (784, 10000), type: Float32\n",
      "X_test min: 0.0, max: 1.0\n",
      "开始训练模型...\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -1.038578, max: 1.2625971\n",
      "Epoch 1: 训练损失 = 0.23097359501063697, 测试损失 = 0.12836263\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -21.276485, max: 20.874569\n",
      "Epoch 2: 训练损失 = 0.11736340255641353, 测试损失 = 0.13810232\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -32.37522, max: 30.033476\n",
      "Epoch 3: 训练损失 = 0.09900698495178875, 测试损失 = 0.11113643\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -30.829212, max: 29.104422\n",
      "Epoch 4: 训练损失 = 0.0838043971770584, 测试损失 = 0.12447511\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -36.67656, max: 30.321447\n",
      "Epoch 5: 训练损失 = 0.0778437289061831, 测试损失 = 0.14410464\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -53.5032, max: 40.95717\n",
      "Epoch 6: 训练损失 = 0.07391274524435623, 测试损失 = 0.121376075\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -50.396866, max: 37.112476\n",
      "Epoch 7: 训练损失 = 0.071182428819459, 测试损失 = 0.15316248\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -50.35532, max: 57.693966\n",
      "Epoch 8: 训练损失 = 0.06847484908334768, 测试损失 = 0.12786789\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -48.522617, max: 56.033306\n",
      "Epoch 9: 训练损失 = 0.0621955403693215, 测试损失 = 0.13323681\n",
      "Batch 1: x_batch shape: (784, 128), y_batch shape: (10, 128)\n",
      "x_batch min: 0.0, max: 1.0\n",
      "Model output shape: (10, 128), min: -59.8602, max: 57.77068\n",
      "Epoch 10: 训练损失 = 0.06039302134270798, 测试损失 = 0.13273162\n",
      "测试模型...\n",
      "测试集准确率: 0.9689\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using MLDatasets\n",
    "using Statistics\n",
    "using Random\n",
    "using Optimisers\n",
    "\n",
    "# 设置随机种子以确保结果可重复\n",
    "Random.seed!(42)\n",
    "\n",
    "# 1. 加载MNIST数据集\n",
    "println(\"加载MNIST数据集...\")\n",
    "train_data = MLDatasets.MNIST(:train)\n",
    "test_data = MLDatasets.MNIST(:test)\n",
    "\n",
    "# 获取训练和测试数据\n",
    "X_train = reshape(train_data.features, :, 60000)\n",
    "y_train = Flux.onehotbatch(train_data.targets, 0:9)\n",
    "X_test = reshape(test_data.features, :, 10000)\n",
    "y_test = Flux.onehotbatch(test_data.targets, 0:9)\n",
    "\n",
    "# 2. 数据预处理\n",
    "println(\"验证原始训练数据：\")\n",
    "println(\"X_train shape: \", size(X_train), \", type: \", eltype(X_train))\n",
    "println(\"X_train min: \", minimum(X_train), \", max: \", maximum(X_train))\n",
    "println(\"验证原始测试数据：\")\n",
    "println(\"X_test shape: \", size(X_test), \", type: \", eltype(X_test))\n",
    "println(\"X_test min: \", minimum(X_test), \", max: \", maximum(X_test))\n",
    "\n",
    "# 仅转换类型，不重复归一化\n",
    "X_train = Float32.(X_train)\n",
    "X_test = Float32.(X_test)\n",
    "\n",
    "# 验证预处理后的数据\n",
    "println(\"预处理后训练数据：\")\n",
    "println(\"X_train shape: \", size(X_train), \", type: \", eltype(X_train))\n",
    "println(\"X_train min: \", minimum(X_train), \", max: \", maximum(X_train))\n",
    "println(\"预处理后测试数据：\")\n",
    "println(\"X_test shape: \", size(X_test), \", type: \", eltype(X_test))\n",
    "println(\"X_test min: \", minimum(X_test), \", max: \", maximum(X_test))\n",
    "\n",
    "# 3. 构建简化的MLP模型（移除softmax）\n",
    "model = Chain(\n",
    "    Dense(784, 128, relu, init=Flux.glorot_uniform),\n",
    "    Dense(128, 64, relu, init=Flux.glorot_uniform),\n",
    "    Dense(64, 10, init=Flux.glorot_uniform)\n",
    ")\n",
    "\n",
    "# 4. 定义损失函数和优化器\n",
    "loss(x, y) = Flux.crossentropy(softmax(model(x)), y)  # 显式softmax\n",
    "opt = Optimisers.Adam(0.01)\n",
    "\n",
    "# 设置优化器状态\n",
    "state = Optimisers.setup(opt, model)\n",
    "\n",
    "# 5. 训练模型\n",
    "println(\"开始训练模型...\")\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "for epoch in 1:num_epochs\n",
    "    # 打乱训练数据\n",
    "    perm = randperm(60000)\n",
    "    X_train = X_train[:, perm]\n",
    "    y_train = y_train[:, perm]\n",
    "    \n",
    "    # 按小批量训练\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i in 1:batch_size:60000\n",
    "        x_batch = X_train[:, i:min(i+batch_size-1, 60000)]\n",
    "        y_batch = y_train[:, i:min(i+batch_size-1, 60000)]\n",
    "        \n",
    "        # 调试：检查批次数据和模型输出\n",
    "        if i == 1\n",
    "            println(\"Batch 1: x_batch shape: \", size(x_batch), \", y_batch shape: \", size(y_batch))\n",
    "            println(\"x_batch min: \", minimum(x_batch), \", max: \", maximum(x_batch))\n",
    "            output = model(x_batch)\n",
    "            println(\"Model output shape: \", size(output), \", min: \", minimum(output), \", max: \", maximum(output))\n",
    "        end\n",
    "        \n",
    "        # 计算梯度和损失\n",
    "        (l, grads) = Flux.withgradient(model) do m\n",
    "            Flux.crossentropy(softmax(m(x_batch)), y_batch)\n",
    "        end\n",
    "        \n",
    "        # 检查梯度和损失\n",
    "        if grads[1] === nothing\n",
    "            println(\"警告：梯度为Nothing，Batch $i\")\n",
    "            continue\n",
    "        elseif isnan(l)\n",
    "            println(\"警告：损失为NaN，Batch $i\")\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        # 更新模型参数\n",
    "        state, model = Optimisers.update(state, model, grads[1])\n",
    "        \n",
    "        total_loss += l\n",
    "        num_batches += 1\n",
    "    end\n",
    "    \n",
    "    # 计算训练集和测试集的平均损失\n",
    "    train_loss = total_loss / num_batches\n",
    "    test_loss = loss(X_test, y_test)\n",
    "    println(\"Epoch $epoch: 训练损失 = $train_loss, 测试损失 = $test_loss\")\n",
    "end\n",
    "\n",
    "# 6. 测试模型\n",
    "println(\"测试模型...\")\n",
    "y_pred = softmax(model(X_test))\n",
    "y_pred_labels = [argmax(y_pred[:, i]) - 1 for i in 1:10000]\n",
    "y_true_labels = test_data.targets\n",
    "accuracy = mean(y_pred_labels .== y_true_labels)\n",
    "println(\"测试集准确率: $accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
